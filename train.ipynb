{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d6d9b4-2219-4250-bcca-4168806b0dd8",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3e1ff3-c229-4f8d-aa7c-a26d03ed4005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ReplayBuffer' from 'F:\\\\Code\\\\Othello_Final\\\\ReplayBuffer.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import Agent\n",
    "import ReplayBuffer\n",
    "import Game\n",
    "\n",
    "# use reload module instead of restarting kernel\n",
    "importlib.reload(Agent)\n",
    "importlib.reload(Game)\n",
    "importlib.reload(ReplayBuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91311b4f-9d8d-41bd-bc79-4d0a486a9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agent import DDQNAgent, RandomAgent, PositionAgent\n",
    "from Game import Othello_vec\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import logging \n",
    "import os, shutil\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def play(agent_black, agent_white, env, epoch=0): \n",
    "    state,round = env.reset()\n",
    "    done = np.full(env.env_nums, False)\n",
    "    cumu_reward = 0 # evaluate時計算所有env平均reward\n",
    "    \n",
    "    while not done.all():\n",
    "        legal_act = env.get_legal_move() # 取得所有環境可以落子的位置 (env_num,action_size)，dytpe:Bool\n",
    "        player = env.get_player() # 目前輪到落子的player \n",
    "\n",
    "        # 該輪落子的環境編號，如果該輪不能下則存在skip_idx\n",
    "        black_idx = (legal_act.any(axis=1)) & (player == -1)\n",
    "        white_idx = (legal_act.any(axis=1)) & (player == 1)\n",
    "        skip_idx = ~legal_act.any(axis=1)\n",
    "\n",
    "        # 記錄各環境落子的位置\n",
    "        action = np.full(env.env_nums,0,dtype=np.int32)   \n",
    "        \n",
    "        if black_idx.any():\n",
    "            # evaluate時以black作為基準對手\n",
    "            action_black = agent_black.get_action(state[black_idx],legal_act[black_idx])\n",
    "            action[black_idx] = action_black\n",
    "        if white_idx.any():\n",
    "            action_white = agent_white.get_action(state[white_idx],legal_act[white_idx])\n",
    "            action[white_idx] = action_white\n",
    "\n",
    "        # 把不能落子的環境跳過一回合\n",
    "        if skip_idx.any():\n",
    "            env.skip_round(skip_idx)\n",
    "            player = env.get_player()\n",
    "            legal_act = env.get_legal_move()\n",
    "\n",
    "            black_idx = legal_act.any(axis=1) & (player == -1) & skip_idx\n",
    "            white_idx = legal_act.any(axis=1) & (player == 1) & skip_idx\n",
    "            done_idx = ~legal_act.any(axis=1) & skip_idx \n",
    "\n",
    "            # 補齊被pass的action\n",
    "            if white_idx.any():\n",
    "                action_white = agent_white.get_action(state[white_idx],legal_act[white_idx])\n",
    "                action[white_idx] = action_white\n",
    "            if black_idx.any():\n",
    "                action_black = agent_black.get_action(state[black_idx],legal_act[black_idx])\n",
    "                action[black_idx] = action_black\n",
    "            if done_idx.any(): # 雙方都不能下，提早結束該局\n",
    "                env.set_done(done_idx)\n",
    "\n",
    "        done,next_state,reward,round,winner,player = env.step(action)\n",
    "        \n",
    "        # update memory & agent (player已經換成對手，所以要再乘-1還原)\n",
    "        black_idx = (player == 1)\n",
    "        white_idx = (player == -1)\n",
    "        if black_idx.any():\n",
    "            agent_black.step(state[black_idx], action[black_idx], reward[black_idx], next_state[black_idx], done[black_idx])\n",
    "        if white_idx.any():\n",
    "            agent_white.step(state[white_idx], action[white_idx], reward[white_idx], next_state[white_idx], done[white_idx])\n",
    "        state = env.get_state()\n",
    "    _,winner = env.get_done_score()\n",
    "    return env.get_done_score()\n",
    "\n",
    "# 用random & position agent測試表現\n",
    "def eval_agent(agent_white,epoch,writer,model_name,max_win_rate):\n",
    "    env = Othello_vec(env_nums=200)\n",
    "    agent_eval = RandomAgent()\n",
    "    _,winner = play(agent_eval,agent_white,env,epoch = epoch)\n",
    "    random_win_rate = sum(winner == 1)/len(winner)\n",
    "    print(f'Evaluate with [Random] \\t win rate:{random_win_rate}')\n",
    "    writer.add_scalar('Win Rate [random]',random_win_rate,epoch)\n",
    "    \n",
    "    env = Othello_vec(env_nums=2)\n",
    "    agent_eval = PositionAgent()\n",
    "    _,winner = play(agent_eval,agent_white,env,epoch = epoch)\n",
    "    position_win_rate = sum(winner == 1)/len(winner)\n",
    "    print(f'Evaluate with [Position] \\t win rate:{position_win_rate}')\n",
    "    writer.add_scalar('Win Rate [position]',position_win_rate,epoch)\n",
    "\n",
    "    if random_win_rate > max_win_rate and position_win_rate == 1:\n",
    "        agent_white.save(f'{model_name}_best_{random_win_rate}.pth')\n",
    "        print(f'save best model {model_name} win_rate:{random_win_rate}')\n",
    "        max_win_rate = random_win_rate\n",
    "    agent_white.save(f'{model_name}_latest.pth')\n",
    "    print('===========================')\n",
    "    return max_win_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f053d-684d-46c6-b56b-6f76d53bf112",
   "metadata": {},
   "source": [
    "### DNN\n",
    "#### 使用reward:\n",
    "* stage 1:\n",
    "    * positional reward\n",
    "    * end game reward\n",
    "    * 避免被佔corner\n",
    "* stage 2: \n",
    "    * 避免被dominate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf4271-0213-4a6a-a21e-558da086b806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'DNN_best_2nd_stage_survive'\n",
    "\n",
    "logging.basicConfig(filename=f'{exp_name}.log', encoding='utf-8', level=logging.INFO)\n",
    "log_dir = f'logs/{exp_name}'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "else:\n",
    "    for root,_,files in os.walk(log_dir):\n",
    "        for f in files:\n",
    "            if '.ipynb_checkpoints' not in f:\n",
    "                os.remove(os.path.join(root,f))\n",
    "                \n",
    "writer = SummaryWriter(log_dir)\n",
    "writer.add_text(\"Experiment Info\",exp_name)\n",
    "\n",
    "# use same agent for self play\n",
    "agent_black = DDQNAgent(eps=0.2,net_type='DNN') \n",
    "agent_white = agent_black\n",
    "\n",
    "epochs = 1000\n",
    "eval_freq = 10\n",
    "env = Othello_vec(env_nums=50)\n",
    "max_win_rate = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    agent_black.set_eps(0.1 + (epochs-epoch)*0.2/epochs)\n",
    "    agent_white.set_eps(0.1 + (epochs-epoch)*0.2/epochs)\n",
    "    done,winner = play(agent_black,agent_white,env)\n",
    "    black,white,round = env.get_info()\n",
    "    \n",
    "    # Evaluation \n",
    "    if epoch%eval_freq == 0:\n",
    "        print(f'[Training] epoch: {epoch} black: {sum(winner == -1)} white: {sum(winner == 1)}')\n",
    "        agent_white.set_eps(0) # for only exploitation\n",
    "        max_win_rate = eval_agent(agent_white,epoch,writer,exp_name,max_win_rate)\n",
    "        writer.add_scalar('average_reward',agent_white.memory.rewards.numpy().mean(),epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae2016-c592-45c0-a13e-14b41118a548",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529f133-2de5-4c5d-9b4a-862459f76d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'CNN_survive_large'\n",
    "\n",
    "logging.basicConfig(filename=f'{exp_name}.log', encoding='utf-8', level=logging.INFO)\n",
    "log_dir = f'logs/{exp_name}'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "else:\n",
    "    for root,_,files in os.walk(log_dir):\n",
    "        for f in files:\n",
    "            if '.ipynb_checkpoints' not in f:\n",
    "                os.remove(os.path.join(root,f))\n",
    "                \n",
    "writer = SummaryWriter(log_dir)\n",
    "writer.add_text(\"Experiment Info\",exp_name)\n",
    "\n",
    "# use same agent for self play\n",
    "agent_black = DDQNAgent(eps=0.2,net_type='CNN') \n",
    "# agent_black.load('./model/CNN_pos+corner_latest-Copy3.pth')\n",
    "agent_white = agent_black\n",
    "\n",
    "epochs = 10000\n",
    "eval_freq = 10\n",
    "env = Othello_vec(env_nums=50)\n",
    "max_win_rate = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    agent_black.set_eps(0.1 + (epochs-epoch)*0.2/epochs)\n",
    "    agent_white.set_eps(0.1 + (epochs-epoch)*0.2/epochs)\n",
    "    done,winner = play(agent_black,agent_white,env)\n",
    "    black,white,round = env.get_info()\n",
    "    \n",
    "    # Evaluation \n",
    "    if epoch%eval_freq == 0:\n",
    "        print(f'[Training] epoch: {epoch} black: {sum(winner == -1)} white: {sum(winner == 1)}')\n",
    "        agent_white.set_eps(0) # for only exploitation\n",
    "        max_win_rate = eval_agent(agent_white,epoch,writer,exp_name,max_win_rate)\n",
    "        writer.add_scalar('average_reward',agent_white.memory.rewards.numpy().mean(),epoch)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
